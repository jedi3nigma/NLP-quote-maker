{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from functions import Rating\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing, threading\n",
    "\n",
    "# pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('extracted_comments.json')\n",
    "df['tag'] = df['tag'].apply(pd.Series)\n",
    "df.drop(['_id', 'rating'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('extracted_comments.json', 'r') as file:\n",
    "#     f = json.load(file)\n",
    "#     print(f[0]['quote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts and average of author/tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4.547224\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "author_cnt = df.groupby('author')['quote'].count().sort_values(ascending=False).rename('count').to_frame()\n",
    "\n",
    "tag_cnt = df['tag'].value_counts().rename('count').sort_values(ascending=False).to_frame()\n",
    "tag_cnt.index.name = 'tag'\n",
    "\n",
    "def barplot_n_bars(df1, df2, n_bars):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,18))\n",
    "    barch1 = sns.barplot(x=df1[:n_bars].index, y='count', data=df1[:n_bars], ax=ax1)\n",
    "    barch2 = sns.barplot(x=df2[:n_bars].index, y='count', data=df2[:n_bars], ax=ax2)\n",
    "    barch1.set_xticklabels(barch1.get_xticklabels(), rotation=90)\n",
    "    barch2.set_xticklabels(barch2.get_xticklabels(), rotation=90)\n",
    "    \n",
    "    mean1 = df1[:n_bars].mean()[0]\n",
    "    mean2 = df2[:n_bars].mean()[0]\n",
    "    \n",
    "    ax1.axhline(mean1, ls='-', color='black')\n",
    "    ax2.axhline(mean2, ls='-', color='black')\n",
    "\n",
    "    ax1.text(n_bars-5, mean1+1.5, f\"Average: {mean1}\", fontsize=16)\n",
    "    ax2.text(n_bars-5, mean2+15, f\"Average: {mean2}\", fontsize=16)\n",
    "\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    \n",
    "barplot_n_bars(author_cnt, tag_cnt, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Rating performance test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_entity(group, cls):\n",
    "    results = list()\n",
    "    for text in group:\n",
    "        results.append((cls.sentiment_rating(text), cls.entity_rating(text)))\n",
    "    return results\n",
    "\n",
    "def compute_ratings(n_items, n_jobs, n_batch_size):\n",
    "    process_rating = Rating()\n",
    "    quotes = process_rating.get_sentences(df['quote'][:n_items])\n",
    "    f = delayed(partial(sentiment_entity, cls=process_rating))\n",
    "    executor = Parallel(n_jobs=n_jobs)\n",
    "    tasks = (f(text_chunk) for text_chunk in minibatch(quotes, size=n_batch_size))\n",
    "    result = executor(tasks)\n",
    "    del process_rating\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_run_time(n_items, n_jobs, n_batches, set_batch_size=True):   \n",
    "    rng_items = np.array(n_items)\n",
    "    rng_jobs = np.array(n_jobs)\n",
    "    if not set_batch_size:\n",
    "        rng_batch_size = np.ceil(np.linspace(1, rng_items.max()/2, n_batches))\n",
    "    else:\n",
    "        rng_batch_size = np.array(n_batches)\n",
    "    params = np.array(np.meshgrid(rng_items, rng_jobs, rng_batch_size)).T.reshape(-1,3)\n",
    "    \n",
    "    runtime_results = list()\n",
    "    for item, job, bsize in params:\n",
    "        start = time.time()\n",
    "        _ = compute_ratings(int(item), int(job), int(bsize))\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        runtime_results.append([item, job, bsize, elapsed])\n",
    "    return runtime_results\n",
    "\n",
    "def plot_run_times(results, run, figsize=(25,10)):\n",
    "    df = pd.DataFrame(results, columns=['Document Count', '# CPU Processes', 'Batch Size', 'Processing Time (s)'])\n",
    "    g = sns.FacetGrid(df, col='# CPU Processes', hue='Batch Size')\n",
    "    g.map_dataframe(sns.lineplot, x='Document Count', y='Processing Time (s)')\n",
    "    g.set_axis_labels('Number documents processed', 'Total elapsed processing time')\n",
    "    g.add_legend()\n",
    "    g.fig.set_size_inches(25,10)\n",
    "    g.savefig('performance_{}_{}.jpg'.format(run, str(n_items[len(n_items)-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# n_items = [1, 50, 100, 500, 1000]\n",
    "# n_jobs = [4, 6, 8, 10]\n",
    "# n_batches = 5\n",
    "\n",
    "# results = calculate_run_time(n_items, n_jobs, n_batches)\n",
    "# plot_run_times(results, 1)\n",
    "\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance = pd.DataFrame(results, columns=['Document Count', '# CPU Processes', 'Batch Size', 'Processing Time (s)'])\n",
    "# performance.to_csv('performance_{}.csv'.format(str(n_items[len(n_items)-1])), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance testing with 8 processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# n_items = [1000, 5000, 10000, 20000]\n",
    "# n_jobs = 8\n",
    "# n_batches = [125, 250]\n",
    "\n",
    "# results2 = calculate_run_time(n_items, n_jobs, n_batches, True)\n",
    "# plot_run_times(results2, 2)\n",
    "\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance2 = pd.DataFrame(results2, columns=['Document Count', '# CPU Processes', 'Batch Size', 'Processing Time (s)'])\n",
    "# performance2.to_csv('performance2_{}.csv'.format(str(n_items[len(n_items)-1])), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Run full results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "full_results = compute_ratings(df.shape[0], 8, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_unravel = [res for results in full_results for res in results]\n",
    "df_results = pd.DataFrame(full_results_unravel, columns=['sentiment', 'entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([df, df_results], axis=1)\n",
    "merged.to_pickle('calculated_ratings.pkl', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
